# Graph-aware DiT configuration for HandSceneGraphDiT
# Controls graph bias, attention, and feed-forward settings.

# Number of stacked HandSceneGraphDiT blocks
depth: 8

# Number of attention heads in each block
num_heads: 8

# Dimension of scene tokens used in cross-attention (defaults to d_model if null)
cross_attention_dim: null

# Feed-forward sub-layer options
ff_inner_dim: null   # Defaults to 4 * d_model when null
ff_bias: true

# Dropout applied in attention and FFN blocks
dropout: 0.0

# Activation function used inside FeedForward blocks
# Options follow diffusers: gelu | geglu | approx_gelu | swiglu
activation_fn: geglu

# Whether to apply qk normalization in attention
qk_norm: false
qk_norm_type: layer

# Normalization type for the main blocks: 'layer' | 'rms'
norm_type: layer

# Epsilon for numerical stability in normalization layers
norm_eps: 1.0e-6

# Structural bias configuration used by GraphAttentionBias
graph_bias:
  enabled: true
  d_struct: 8

# Optional hand->scene distance bias added to cross-attention logits
# Uses squared distance for smoother decay: bias = -gamma * dist²
# - Near points: fine-grained differentiation
# - Far points: rapid falloff
hand_scene_bias:
  enabled: false   # default off for backward compatibility
  gamma: 0.5       # scale for negative squared distance bias (bias = -gamma * dist²)
