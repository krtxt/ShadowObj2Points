# Model Configuration for HandFlowMatchingDiT (Flow Matching + Graph-aware DiT)
# @package model
#
# This config defines the LightningModule and lets Hydra recursively
# instantiate all sub-modules (backbone, hand encoder, DiT, loss, optimizer).
# Graph constants (graph_consts) are provided at runtime by the DataModule
# via HandEncoderDataModule.get_graph_constants().

_target_: models.flow_matching_hand_dit.HandFlowMatchingDiT

# Note: graph_consts will be injected at runtime by train.py

# -----------------------------------------------------------------------------
# 1. Core architecture hyperparameters
# -----------------------------------------------------------------------------
# d_model: token/channel dimension shared across sub-modules.
#          Must match the backbone out_dim and the hand encoder output dim.
d_model: 512

# -----------------------------------------------------------------------------
# 2. Sub-module configuration (hand encoder, backbone, DiT)
# -----------------------------------------------------------------------------
# These point to separate Hydra config groups so that components can be swapped
# via command line overrides (e.g., backbone=ptv3_sparse).
#
# When Hydra instantiates model=flow_matching_hand_dit with _recursive_=true,
# the following nested configs are instantiated automatically and passed as
# module instances to HandFlowMatchingDiT.__init__:
#
#   backbone     <- ${backbone}
#   hand_encoder <- ${hand_encoder}
#   dit          <- ${dit}
#
backbone: ${backbone}
hand_encoder: ${hand_encoder}
dit: ${dit}

# -----------------------------------------------------------------------------
# 2.1 Velocity Strategy & State Projection
# -----------------------------------------------------------------------------
# Controlled by configs/velocity_strategy/*.yaml
velocity_mode: ${velocity_strategy.mode}
velocity_kwargs: ${velocity_strategy.kwargs}
state_projection_mode: ${velocity_strategy.projection_mode}
state_projection_kwargs: ${velocity_strategy.projection_kwargs}


# -----------------------------------------------------------------------------
# 3. Flow Matching loss
# -----------------------------------------------------------------------------
# FlowMatchingLoss is defined in src/models/components/losses.py and receives
# edge_index + lambda_tangent. We pass the loss config through so the model can
# instantiate it (and inject graph constants) via Hydra.
#
# To change lambda_tangent, override:
#   loss=flow_matching loss.lambda_tangent=0.5
loss_cfg: ${loss}

# -----------------------------------------------------------------------------
# 4. Optimization (optimizer + scheduler)
# -----------------------------------------------------------------------------
# Optimizer and scheduler are configured via configs/optimizer/*.yaml.
# The full optimizer config (including nested `scheduler`) is passed through
# and instantiated inside HandFlowMatchingDiT.configure_optimizers().
optimizer_cfg: ${optimizer}

# -----------------------------------------------------------------------------
# 5. Sampling & constraint projection
# -----------------------------------------------------------------------------
# sample_num_steps: 采样时 ODE Euler 步数（从 t=0 积分到 t=1）
#   - 20: 快速粗略采样
#   - 40: 默认推荐
#   - 60+: 更细粒度积分，推理更慢
sample_num_steps: 20

# sample_solver: 采样时使用的数值积分器
#   - "euler": 显式 Euler（默认）
#   - "heun": 二阶 Heun / RK2（更精确，每步多一次网络前向）
#   - "rk4": 四阶 Runge-Kutta（最高精度，每步 4 次网络前向）
sample_solver: rk4


# 调度策略
sample_schedule: shift 

# 偏移量 (Shift)
# 1.0 = 线性
# > 1.0 (如 3.0) = 关注高频细节/数据端 (推荐用于高分辨率或复杂几何)
# < 1.0 = 关注整体结构/噪声端
schedule_shift: 3.0

# proj_num_iters: 每个采样时间步中 PBD 边长投影的迭代次数
#   - 0: 不做显式投影，只依赖网络和切向损失
#   - 1–3: 通常足够（默认 2）
proj_num_iters: 2

# proj_max_corr: 每次投影的最大相对修正幅度（防止数值不稳定）
proj_max_corr: 0.2

# EMA & Visualization moved to callbacks
