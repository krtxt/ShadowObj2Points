# Model Configuration for HandFlowMatchingDiT (Flow Matching + Graph-aware DiT)
# This config defines all static hyperparameters for the flow-matching hand DiT model.
# Graph constants (graph_consts) are expected to be provided at runtime by the DataModule
# via HandEncoderDataModule.get_graph_constants().

_target_: src.models.flow_matching_hand_dit.HandFlowMatchingDiT

# Note: graph_consts will be injected at runtime by train.py

# -----------------------------------------------------------------------------
# 1. Core architecture hyperparameters
# -----------------------------------------------------------------------------
# d_model: token/channel dimension shared across sub-modules.
#          Must match the backbone out_dim and the hand encoder output dim.
d_model: 512

# -----------------------------------------------------------------------------
# 2. Sub-module configuration (hand encoder, backbone, DiT)
# -----------------------------------------------------------------------------
# These are configured via separate Hydra config groups:
#   - backbone: configs/backbone/*.yaml (e.g., ptv3_sparse_perceiver)
#   - hand_encoder: configs/hand_encoder/*.yaml (e.g., transformer_bias, egnn_lite)
#   - dit: configs/dit/*.yaml (e.g., graph_dit_default)
#
# train.py wires these groups via dedicated builder functions before instantiating
# HandFlowMatchingDiT:
#   backbone = build_backbone(cfg.backbone)
#   hand_encoder = build_hand_encoder(cfg.hand_encoder, graph_consts, out_dim=d_model)
#   dit = build_dit(cfg.dit, graph_consts, dim=d_model)
# so each component can be swapped via the defaults list without touching the model code.

# -----------------------------------------------------------------------------
# 3. Training / loss behaviour
# -----------------------------------------------------------------------------
# FlowMatchingLoss is defined in src/models/components/losses.py and receives
# edge_index + lambda_tangent. That module is created inside
# HandFlowMatchingDiT, so only the scalar weight needs to be configured here.

# -----------------------------------------------------------------------------
# 4. Flow Matching loss & regularization
# -----------------------------------------------------------------------------
# lambda_tangent: 切向约束损失权重
#   - 0.0: 只优化 flow matching loss，不做边长切向约束
#   - 1.0: 默认，兼顾生成质量与物理一致性
#   - 更高的值会更严格地约束边长，但可能略微影响多样性
lambda_tangent: 1.0

# -----------------------------------------------------------------------------
# 5. Optimization hyperparameters
# -----------------------------------------------------------------------------
# learning_rate: AdamW 学习率（HandFlowMatchingDiT.configure_optimizers 中使用）
learning_rate: 3e-4

# weight_decay: AdamW 权重衰减系数
weight_decay: 1e-2

# use_scheduler: 是否启用 CosineAnnealingLR 学习率调度器
use_scheduler: true

# scheduler_T_max: CosineAnnealingLR 的 T_max（以 epoch 为单位）
scheduler_T_max: 100

# -----------------------------------------------------------------------------
# 6. Sampling & constraint projection
# -----------------------------------------------------------------------------
# sample_num_steps: 采样时 ODE Euler 步数（从 t=0 积分到 t=1）
#   - 20: 快速粗略采样
#   - 40: 默认推荐
#   - 60+: 更细粒度积分，推理更慢
sample_num_steps: 40

# sample_solver: 采样时使用的数值积分器
#   - "euler": 显式 Euler（默认）
#   - "heun": 二阶 Heun / RK2（更精确，每步多一次网络前向）
sample_solver: euler

# proj_num_iters: 每个采样时间步中 PBD 边长投影的迭代次数
#   - 0: 不做显式投影，只依赖网络和切向损失
#   - 1–3: 通常足够（默认 2）
proj_num_iters: 2

# proj_max_corr: 每次投影的最大相对修正幅度（防止数值不稳定）
proj_max_corr: 0.2
