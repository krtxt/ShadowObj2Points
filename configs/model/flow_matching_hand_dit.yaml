# Model Configuration for HandFlowMatchingDiT (Flow Matching + Graph-aware DiT)
# @package model
#
# This config defines the LightningModule and lets Hydra recursively
# instantiate all sub-modules (backbone, hand encoder, DiT, loss, optimizer).
# Graph constants (graph_consts) are provided at runtime by the DataModule
# via HandEncoderDataModule.get_graph_constants().

_target_: models.flow_matching_hand_dit.HandFlowMatchingDiT

# Note: graph_consts will be injected at runtime by train.py

# -----------------------------------------------------------------------------
# 1. Core architecture hyperparameters
# -----------------------------------------------------------------------------
# d_model: token/channel dimension shared across sub-modules.
#          Must match the backbone out_dim and the hand encoder output dim.
d_model: 512

# -----------------------------------------------------------------------------
# 2. Sub-module configuration (hand encoder, backbone, DiT)
# -----------------------------------------------------------------------------
# These point to separate Hydra config groups so that components can be swapped
# via command line overrides (e.g., backbone=ptv3_sparse).
#
# When Hydra instantiates model=flow_matching_hand_dit with _recursive_=true,
# the following nested configs are instantiated automatically and passed as
# module instances to HandFlowMatchingDiT.__init__:
#
#   backbone     <- ${backbone}
#   hand_encoder <- ${hand_encoder}
#   dit          <- ${dit}
#
backbone: ${backbone}
hand_encoder: ${hand_encoder}
dit: ${dit}

# -----------------------------------------------------------------------------
# 2.1 Velocity Strategy & State Projection
# -----------------------------------------------------------------------------
# Controlled by configs/velocity_strategy/*.yaml
velocity_mode: ${velocity_strategy.mode}
velocity_kwargs: ${velocity_strategy.kwargs}
state_projection_mode: ${velocity_strategy.projection_mode}
state_projection_kwargs: ${velocity_strategy.projection_kwargs}

# -----------------------------------------------------------------------------
# 2.2 Prediction Target (JiT-style option)
# -----------------------------------------------------------------------------
# prediction_target: 网络输出的语义，参考 JiT (arXiv:2411.xxx) 的研究结论
#   - "v": 直接预测速度 v (默认，传统 Flow Matching)
#   - "x": 预测目标位置 x，然后反推速度 v = (x_pred - z) / (1-t)
#          JiT 发现预测 x 可以获得更好的生成质量
# 此选项与 velocity_strategy 正交，可自由组合
prediction_target: v

# tau_min: 当 prediction_target='x' 时，防止 t→1 除零的最小值
#   - 1e-3: 默认值，较保守
#   - 1e-5: JiT 论文使用的值，更激进
#   - 如果训练末期不稳定，可以增大此值
tau_min: 1e-3


# -----------------------------------------------------------------------------
# 3. Flow Matching loss
# -----------------------------------------------------------------------------
# FlowMatchingLoss is defined in src/models/components/losses.py and receives
# edge_index + lambda_tangent. We pass the loss config through so the model can
# instantiate it (and inject graph constants) via Hydra.
#
# To change lambda_tangent, override:
#   loss=flow_matching loss.lambda_tangent=0.5
loss_cfg: ${loss}

# -----------------------------------------------------------------------------
# 4. Optimization (optimizer + scheduler)
# -----------------------------------------------------------------------------
# Optimizer and scheduler are configured via configs/optimizer/*.yaml.
# The full optimizer config (including nested `scheduler`) is passed through
# and instantiated inside HandFlowMatchingDiT.configure_optimizers().
optimizer_cfg: ${optimizer}

# -----------------------------------------------------------------------------
# 5. Sampling & constraint projection
# -----------------------------------------------------------------------------
use_norm_data: true
use_per_point_gaussian_noise: true

# sample_num_steps: 采样时 ODE Euler 步数（从 t=0 积分到 t=1）
#   - 20: 快速粗略采样
#   - 40: 默认推荐
#   - 60+: 更细粒度积分，推理更慢
sample_num_steps: 20

# sample_solver: 采样时使用的数值积分器
#   - "euler": 显式 Euler（默认）
#   - "heun": 二阶 Heun / RK2（更精确，每步多一次网络前向）
#   - "rk4": 四阶 Runge-Kutta（最高精度，每步 4 次网络前向）
sample_solver: rk4


# 调度策略
sample_schedule: shift 

# 偏移量 (Shift)
# 1.0 = 线性
# > 1.0 (如 3.0) = 关注高频细节/数据端 (推荐用于高分辨率或复杂几何)
# < 1.0 = 关注整体结构/噪声端
schedule_shift: 3.0

# proj_num_iters: 每个采样时间步中 PBD 边长投影的迭代次数
#   - 0: 不做显式投影，只依赖网络和切向损失
#   - 1–3: 通常足够（默认 2）
proj_num_iters: 2

# proj_max_corr: 每次投影的最大相对修正幅度（防止数值不稳定）
proj_max_corr: 0.2

# -----------------------------------------------------------------------------
# 6. Validation sampling config (memory optimization)
# -----------------------------------------------------------------------------
# 验证时的 recon metrics 需要完整 ODE 采样，开销很大。
# 指定想要采样的 batch 数量，系统会自动均匀分布在整个验证集上。
#
# val_num_sample_batches: 验证时采样多少个 batch 来计算 recon metrics
#   - 自动计算均匀间隔: stride = total_val_batches / val_num_sample_batches
#   - 例如: 验证集 100 batch, val_num_sample_batches=10 → 每 10 个 batch 采样一次
#   - 第一个 epoch 会自动探测验证集大小，之后均匀采样
#   - 总采样样本数 = val_num_sample_batches * batch_size
val_num_sample_batches: 10

