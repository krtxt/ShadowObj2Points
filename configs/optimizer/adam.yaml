# Adam Optimizer Configuration (supports Hydra _target_ or legacy type)
_target_: torch.optim.Adam
lr: 1.0e-4
weight_decay: 0.0
betas: [0.9, 0.999]
eps: 1.0e-8

scheduler:
  type: cosine
  t_max: ${trainer.max_epochs}
  eta_min: 1.0e-6
  interval: epoch
  frequency: 1
  monitor: val/loss
