# AdamW Optimizer Configuration (supports both Hydra _target_ and legacy 'type' style)
# Preferred Hydra style:
_target_: torch.optim.AdamW
lr: 1.0e-4
weight_decay: 0.01
betas: [0.9, 0.999]
eps: 1.0e-8
foreach: false

scheduler:
  type: warmup_cosine
  warmup_epochs: 10
  total_epochs: ${trainer.max_epochs}
  eta_min: 1.0e-6
  warmup_start_factor: 1.0e-3
  interval: epoch
  frequency: 1
  monitor: val/loss
