# AdamW Optimizer Configuration (Hydra style)
_target_: torch.optim.AdamW
lr: 1.0e-4
weight_decay: 0.01
betas: [0.9, 0.999]
eps: 1.0e-8
foreach: false

# Scheduler config is nested under `scheduler` and will be instantiated
# via Hydra inside HandFlowMatchingDiT.configure_optimizers().
scheduler:
  _target_: models.components.schedulers.build_warmup_cosine_scheduler
  warmup_epochs: 10
  total_epochs: ${trainer.max_epochs}
  eta_min: 1.0e-6
  warmup_start_factor: 1.0e-3
  interval: epoch
  frequency: 1
  monitor: val/loss
