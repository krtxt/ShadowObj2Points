# AdamW Optimizer Configuration (supports both Hydra _target_ and legacy 'type' style)
# Preferred Hydra style:
_target_: torch.optim.AdamW
lr: 1.0e-4
weight_decay: 0.01
betas: [0.9, 0.999]
foreach: false

# Optional scheduler: prefer Hydra style if possible
# Example (CosineAnnealingLR):
# scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#   T_max: 100
#   eta_min: 1.0e-6

type: adamw
scheduler:
  type: warmup_cosine  # 'cosine' | 'step' | 'reduce_on_plateau' | 'warmup_cosine' | 'none'
  warmup_epochs: 10
  total_epochs: ${trainer.max_epochs}
  # For cosine
  # t_max: 100
  # eta_min: 1.0e-6
  # For step
  # step_size: 30
  # gamma: 0.1
  # For reduce_on_plateau
  # factor: 0.5
  # patience: 10
  # min_lr: 1.0e-6
  # monitor: val/total
